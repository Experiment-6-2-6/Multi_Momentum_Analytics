---
title: "Dual Momentum Analysis"
author: "Kristoff Dudek"
date: "2022-11-26"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: TRUE
---
*This document is a record of my thought processes and a documentation of the workflow of the analysis.*

*Also, it is a presentation of my data analyst skills addressed to my future clients/employers.*

*Hence, please remember, it is for information purposes only and does not consist of investment advice.*

---

# Introduction

I based my investment strategy on Gary Antonacci's "Dual-Momentum". It is explained by him in the book "Dual Momentum Investing: An Innovative Strategy for Higher Returns with Lower Risk" (16 Dec. 2014 McGraw Hill). You can find a short YouTube video with the author explaining the strategy [here](https://www.youtube.com/watch?v=lf1u4dsHavs).

I am interested in making some changes to the strategy and test if it works with crypto or with single stocks pre-filtered by ROIC-EPSgr-BVgr-REVgr-Debt. However, it is going to take a considerable chunk of my free time so I decided to divide the analysis in parts and treat them as independent and complete modules.

Simultaneously, I didn't want to waste such a good piece of analytical work and I decided to use it to show my abilities to potential clients/employers. To suit this goal, this document became extremely verbose and now looks more like a tutorial than my usual analysis.

For the same reason I use an external database even if there is no real need to do so to produce a comprehensive report but the use of databases allows me to show my SQL skills and simultaneously it creates data sources for Tableau and MS Power BI presentations which I am going to create later.

If you are not interested in my thought process, reasoning and workflow and you only want to see the practical version of this work in the action please see the dm_r_code_only.R file [here](https://colab.research.google.com/drive/1dSvAo9at6_AJCSfZQDgrQ5BLYH9FAY2V?usp=sharing). The code is commented and it shouldn't be challenging to anyone with a knowledge of R.

At the beginning of this work I wanted to build this notebook as usual. That means some thoughts followed by code followed by explanations. But soon I realised that it will create unnecessary distraction for non-coding audiences. So finally I have put all explanatory parts at the beginning of the notebook and the code of the first module at the end. The both parts are in chronological order and have similar titles. I hope this way the work is easier to read and to understand.

I write under a version control using GitHub and you can access the repository of the project [here](https://github.com/Experiment-6-2-6/Multi_Momentum_Analytics)

If you have any questions please contact me via LinkedIn [here](https://www.linkedin.com/in/kristoffdudek/) or via an email [click here](mailto:kriss.dudek@gmail.com) and send it using your mail client.

So if we finished the introduction, let's dive into the project...

# Questions

I am not going to repeat the research of Mr Antonacci but I want to test if his system can be improved upon and if it works with different assets. There are some obvious weaknesses in the dual-momentum for example in a high volatile environment it can miss the price move by a lot because the check is once a month. So maybe I can find a way to improve it - at least for me.

My questions are:

* What if we check the dual-momentum each week instead of month?
* How that influences the turnover and the trading costs. 
* Can it work better with a bigger number of ETFs?
* Is it possible to minimise drawdown by using EMA(30W) as an absolute momentum measure?
* Is it actually more efficient than the optimised basket of all ETFs used in it?
* How does it work with a basket of single stocks (prefiltered by my usual rules of a good investment company)?
* How does it work with crypto?
* What happens if we use monthly instead of yearly momentum?

# Assumptions and goals

* All data should be pulled from public, free sources.
* The analyses will be performed at weekends only.
* I am going to get data not older than 18 April 2013 because only after this date a commission free trading became available.
* All prices of ETFs, shares and crypto will be in USD as I want to avoid additional variables of GBP gain/loss. 
* If possible, ETFs should be listed on London Stock Exchange and if possible have their version in GBX as the smallest known currency conversion fee is 0.15% mark-up and I would rather avoid that in future application.
* For comparison I need to do some data mining and get a table of weekly performances including:
  + Standard Dual Momentum Strategy.
  + DM applied to the basket of 10 ETF.
  + DM applied to the basket of 5 shares.
  + DM applied to 10 crypto.
* Each of these options must have its version with:
  + Check - Rebalancing done monthly
  + Check - Rebalancing done weekly
* And these version consecutively needs variants with absolute momentum calculated:
  + Using just positive or negative (sign).
  + Using Exponential Moving Average of 30 weeks.

That gives 16 combinations plus one additional for performance of an optimised basket of ETFs.
* To get this I need prepare some R scripts:
  + Basic Dual Momentum Assessment and use EMA(30) (module I)
  + Portfolio Performance Valuation (module II)
  + Portfolio Optimisation (module III)
  + Share Screen (module IV)
* For the module I the summary should be in a form of leaderboard table with ETFs sorted from top to bottom the strongest momentum on top.
* Additionally, I would like to add the weekly ETF momentum calculations to my Google Sheets.
* I am interested in weekly data only but I will keep cleaned daily data and daily calculations in my database for (Tableau and Power BI).
* For the relative momentum I am going to compare the 1 year rate of change in the ETFs performance.
* In module I I am going to implement EMA(30) as a measure of the absolute momentum as it is going to be more complicated than simply looking if the momentum is positive or negative.
* I think that the last 3 years of data would suffice for my needs only. And I am not very interested in how good the system was in the past. I am going to focus on my practical goals.
* For the research I am going to use the following baskets of assets:

**Classic Dual-Momentum Case:**

1. **VOO** Vanguard S&P 500 ETF
1. **VEO** Vanguard All-World ex-US ETF
1. **BND** Vanguard Total Bond Market Index ETF

**10 ETFs from London Stock Exchange**

1. **VUAA**	Vanguard S&P 500 UCITS ETF USD (acc)
1. **CNDX**	iShares NASDAQ 100 UCITS ETF USD (acc)
1. **VWRA**	Vanguard FTSE All-World UCITS ETF USD (acc)
1. **IWMO**	iShares Edge MSCI World Momentum Factor UCITS ETF USD (acc)
1. **IWQU**	iShares Edge MSCI World Quality Factor UCITS ETF USD (acc)
1. **IWVL**	IShares Edge MSCI World Value Factor UCITS ETF USD (acc)
1. **WSML**	iShares MSCI World Small Cap UCITS ETF USD (acc)
1. **VFEA**	Vanguard FTSE Emerging Mkts UCITS ETF USD (acc)
1. **IGIL**	iShares Global Inflation Linked Govt Bonds UCITS ETF USD (acc)
1. **IGLN**	iShares Physical Gold ETC USD (no income)

**Crypto Basket**

1. AAVE
1. Cardano
1. Cosmos
1. Avalanche
1. Binance Coin
1. Bitcoin
1. Pancake
1. CatGirl
1. DogeCoin
1. PolkaDot
1. Ethereum
1. ChainLink
1. LiteCoin
1. Mana
1. Matic
1. NEAR
1. PAX Gold
1. Shiba Inu
1. Sushi
1. TON Coin
1. TRON
1. Unicorn Token

# Methodology and Workflow

Next few paragraphs are explanations of the code and the final workflow in each consecutive module.
 
If you want to understand how the project is going and what problems and ideas I have (had) it can be better if you first read the "Issue Register" as it is an index of all problems, issues, ideas, changes and it is a description of the workflow in the project as a whole. 

## Module One: Data Mining

I can use any of my baskets but I decided to work out the code using the set of 10 ETFs with a 30 week exponential moving average.

### Environment Requirements

I am using following packages:

* Quantmod for quantitative analysis
* Tidyverse for general data manipulation
* Dplyr which is a part of Tidyverse but needs to be activated separately
* RPostgres for working with databases (do not use RPostgreSQL as it causes problems with a SSL connections)
* GT to create a Great Table view in final report
* Reshape2 to transform data into long format to simplify plot drawing
* Ggplot2 for plotting how to take over the world in 3 easy steps
* Googlesheets4 to write to my Google sheet

### Getting and cleaning data

* Establishing the connection with my database
* Getting the list of the ETFs names and symbols from the db
* Limiting data pull to the last 3 years
* Populating the daily and weekly data frames:
  + getting data from Yahoo Finance
  + extracting only the adjusted daily closes
  + naming the columns with ETF names
  + dropping any rows with NA
  + transforming row names into date column (for use in SQL)
* Rewriting the quotes_usd database as I want to have 3 years of daily closing prices on my server for the later use.

### Additional Explanations

Instead of just updating the data in the table I am dropping it completely because my list of ETFs can change with time so the database has to adapt dynamically. And because I want the last 3 years of data only it is much simpler to rewrite the whole DB from a scratch.


I have decided to drop whole rows from df if there is any NA in it because I need the ETFs' momentum in relation to each other. That means it doesn't matter much if sometimes I have longer or shorter periods for comparison as long as the periods are the same.


I had to split XTS columns (eXtensive Time Series) into date and float to make them compatible with SQL database.


When calculating ROC, I had chosen the simple Rate Of Change calculation method because I want it to be between -100% and +100% (and not -âˆž to +âˆž). It is worth remembering that in ROC() function from Quantmod:

* ROC() "discrete" = simple (arithmetic) return,
* ROC() "continuous" = logarithmic (geometric) method

### Calculations

* checking if the asset's price is above the EMA(30)
* calculating 1 year ROC
* Transforming the result into a _common format_ (percentage with 2 decimals only).
* Sending the results to the database.

### Table and Plot (Report)

Now it is time to build the report.
As I already have sent all necessary data to the DB I can close the connection and focus on the local data frames.

I want the report to be in the form of a table and a graph. To get that I am:

* Sorting columns by the momentum of the last week in descending order.
* Preparing the leaderboard:
  + Building the table from the components
  + Transposing the momentum table to vertical view
  + Formatting and adding columns names
  + Printing the table
* Preparing the last year momentum chart
  + Transforming selected data into long format
  + Building the plot
  + Drawing it

### Exporting CSV files.

For convenience sake I added a possibility to export the data in the csv format. The csv files are very easy to import into excel if one wishes to do so.

### Adding output to Google Sheet.

This part is rather for my convenience and I don't recommend running this chunk of code but I made the file publicly available for others (view mode only). You can always access it by clicking [here](https://docs.google.com/spreadsheets/d/1ULTKECt44zBzhROLlXMx3XtD-Vm0M5kQ2Ve63AtzJ64/edit?usp=sharing).

It is only a single file because I had exported these two dataframes into one Google spreadsheet but into two separate sheets. The first one is a summary table and the second is hidden. Instead of showing it I use its data to create a chart on another (visible) sheet.

## Module I - Code and Output

The example output shown here uses:

* selection of 10 ETFs
* relative momentum of a year
* absolute momentum judged by EMA(30)
* checking / rebalancing period of a month

```{r, label = 'packages', echo = TRUE, eval = FALSE}
# Packages ---------------------------------------------------------------------
install.packages("quantmod")
install.packages("tidyverse")
install.packages("RPostgres")
install.packages("gt")
install.packages("reshape2")
install.packages("ggplot2")
```
```{r, 'libraries', echo = TRUE, message = FALSE, warning = FALSE}
# Libraries --------------------------------------------------------------------
library(quantmod)
library(tidyverse)
library(dplyr)
library(RPostgres)
library(gt)
library(reshape2)
library(ggplot2)
library(googlesheets4)
```
```{r, 'Connection-On', echo = TRUE, message = FALSE, warning = FALSE}

# Postgres DB connection -------------------------------------------------------
my_postgr <- dbConnect(
  RPostgres::Postgres(),
  host = Sys.getenv("sql_host"),
  dbname = Sys.getenv("sql_db_name"),
  user = Sys.getenv("sql_user"),
  password = Sys.getenv("sql_password"),
  port = 5432,
  sslmode = "require"
)
```
```{r, 'Getting-Data', echo = TRUE, results = FALSE}
# Getting the Data -------------------------------------------------------------
start_date <- Sys.Date() - (3 * 365)
end_date <- Sys.Date()
etf_qts_env <- new.env()
etf_query <- dbGetQuery(my_postgr, "SELECT * FROM etf_info;")
yahoo_symbols <- as.vector(etf_query[["symbol_usd"]])

# daily quotes
getSymbols(yahoo_symbols, 
           from = start_date, 
           to = end_date, 
           periodicity = "daily",
           env = etf_qts_env)
daily_closes <- do.call(merge, eapply(etf_qts_env, Ad))
daily_closes <- na.omit(daily_closes)
names(daily_closes) <- gsub(".L.Adjusted", "", names(daily_closes))

# weekly quotes
getSymbols(yahoo_symbols, 
           from = start_date, 
           to = end_date, 
           periodicity = "weekly",
           env = etf_qts_env)
weekly_closes <- do.call(merge, eapply(etf_qts_env, Ad))
weekly_closes <- na.omit(weekly_closes)
names(weekly_closes) <- gsub(".L.Adjusted", "", names(weekly_closes))

# monthly quotes
getSymbols(yahoo_symbols, 
           from = start_date, 
           to = end_date, 
           periodicity = "monthly",
           env = etf_qts_env)
monthly_closes <- do.call(merge, eapply(etf_qts_env, Ad))
monthly_closes <- na.omit(monthly_closes)
names(monthly_closes) <- gsub(".L.Adjusted", "", names(monthly_closes))
```
```{r, 'Quotes-to-DB', echo = TRUE, message = FALSE, warning = FALSE}
# Writing into DB (quotes) -----------------------------------------------------
# Have to transform into PostgreSQL format before sending to DB
dc_for_sql <- as.data.frame(daily_closes)
dc_for_sql <- cbind(date = as.Date(rownames(dc_for_sql)), dc_for_sql)
if (dbExistsTable(my_postgr, "quotes_etfs_d")) {
  dbRemoveTable(my_postgr, "quotes_etfs_d")
}
dbWriteTable(my_postgr, "quotes_etfs_d", dc_for_sql, row.names = FALSE)

wc_for_sql <- as.data.frame(weekly_closes)
wc_for_sql <- cbind(date = as.Date(rownames(wc_for_sql)), wc_for_sql)
if (dbExistsTable(my_postgr, "quotes_etfs_w")){
  dbRemoveTable(my_postgr, "quotes_etfs_w")
}
dbWriteTable(my_postgr, "quotes_etfs_w", wc_for_sql, row.names = FALSE)

mc_for_sql <- as.data.frame(monthly_closes)
mc_for_sql <- cbind(date = as.Date(rownames(mc_for_sql)), mc_for_sql)
if (dbExistsTable(my_postgr, "quotes_etfs_m")){
  dbRemoveTable(my_postgr, "quotes_etfs_m")
}
dbWriteTable(my_postgr, "quotes_etfs_m", mc_for_sql, row.names = FALSE)
```
```{r, 'Absolute-Mmt', echo = TRUE, message = FALSE, warning = FALSE}
# Absolute Momentum Check ------------------------------------------------------
mm_direction <- rep(TRUE, ncol(weekly_closes))
names(mm_direction) <- names(weekly_closes)
for (ticker in names(weekly_closes)){
  ema30w <- EMA(weekly_closes[,ticker], n = 30)
  if (last(ema30w) < last(weekly_closes[,ticker])) {
    mm_direction[[ticker]] <- TRUE
  } else {
      mm_direction[[ticker]] <- FALSE
    }
}
```
```{r, 'Relative-Mmt', echo = TRUE, message = FALSE, warning = FALSE}
# Relative Momentum Calculation ------------------------------------------------
daily_1ym <- ROC(daily_closes, n = 252, type = "discrete") %>%  
              na.omit() %>% "*"(100) %>% round(2)
weekly_1ym <- ROC(weekly_closes, n = 50, type = "discrete") %>%  
                na.omit() %>% "*"(100) %>% round(2)
monthly_1ym <- ROC(monthly_closes, n = 12, type = "discrete") %>% 
                 na.omit() %>% "*"(100) %>% round(2)
```
```{r, 'Momentum-to-DB', echo = TRUE, message = FALSE, warning = FALSE}
# Writing into DB (momentum) ---------------------------------------------------
dm_for_sql <- as.data.frame(daily_1ym)
dm_for_sql <- cbind(date = as.Date(rownames(dm_for_sql)), dm_for_sql)
if (dbExistsTable(my_postgr, "momentum_etfs_d")) {
  dbRemoveTable(my_postgr, "momentum_etfs_d")
}
dbWriteTable(my_postgr, "momentum_etfs_d", dm_for_sql, row.names = FALSE)

wm_for_sql <- as.data.frame(weekly_1ym)
wm_for_sql <- cbind(date = as.Date(rownames(wm_for_sql)), wm_for_sql)
if (dbExistsTable(my_postgr, "momentum_etfs_w")) {
  dbRemoveTable(my_postgr, "momentum_etfs_w")
}
dbWriteTable(my_postgr, "momentum_etfs_w", wm_for_sql, row.names = FALSE)

mm_for_sql <- as.data.frame(monthly_1ym)
mm_for_sql <- cbind(date = as.Date(rownames(mm_for_sql)), mm_for_sql)
if (dbExistsTable(my_postgr, "momentum_etfs_m")) {
  dbRemoveTable(my_postgr, "momentum_etfs_m")
}
dbWriteTable(my_postgr, "momentum_etfs_m", mm_for_sql, row.names = FALSE)
```
```{r, 'Connection-Off', echo = TRUE, message = FALSE, warning = FALSE}
# Connection Off ---------------------------------------------------------------
dbDisconnect(my_postgr)
```
```{r, 'Table', echo = TRUE, message = FALSE, warning = FALSE}
# Leaderboard Table ------------------------------------------------------------
temp_df <- as_tibble(monthly_1ym)
temp_df <- temp_df[,order(temp_df[nrow(temp_df),],decreasing = TRUE)]
temp_df <- tail(temp_df, 1)

descriptions <- NULL
for (ticker in names(temp_df)) {
  description <- etf_query$full_name[etf_query$ticker_usd == ticker]
  descriptions <- append(descriptions, description)
}

leaderboard_data <- tibble(descriptions,
                           names(temp_df),
                           mm_direction,
                           t(temp_df))

names(leaderboard_data) <- c("etf_name",
                             "USD", "Above_EMA", "Y_Mm")

leaderboard_table <-  
  gt(leaderboard_data) %>% 
  tab_header(title = md("**ETFs Sorted By 1 Year Momentum**"), 
             subtitle = "last 2 years of data") %>% 
  tab_source_note(md("_datasource: www.yahoo.com_")) %>% 
  tab_style(style = list(cell_text(color = "#196F3D")),
    locations = cells_body(columns = Above_EMA, rows = Above_EMA == TRUE)) %>% 
  tab_style(style = list(cell_text(color = "#7B241C")),
    locations = cells_body(columns = Above_EMA, rows = Above_EMA == FALSE)) %>% 
  tab_style(style = list(cell_fill(color = "#E8F8F5"),
                         cell_text(weight =  "bold")),
    locations = cells_body(rows = 1)) %>% 
  tab_style(style = cell_text(align = "right"),
    locations = cells_source_notes()) %>%
  cols_label(etf_name = "ETF Full Name / Description", 
             USD = "Ticker",
             Above_EMA = "Above EMA", 
             Y_Mm = "1Y Mm %")
  
leaderboard_table
```
```{r, 'Plot', echo = TRUE, warning = FALSE, out.width= '100%', fig.asp = '1.618', dpi = 96}
# Plot -------------------------------------------------------------------------
temp_df <- select(wm_for_sql, -date)
temp_df <- temp_df[,order(temp_df[nrow(temp_df),],decreasing = TRUE)]
temp_df <- select(temp_df, 1:3)
temp_df <- tibble(date = wm_for_sql$date, temp_df)
temp_df <- tail(temp_df, 24)
plot_data <- melt(temp_df, id = "date")

best_3_plot <- ggplot(plot_data, aes(x = date, y = value, colour = variable)) +
  geom_line() +
  labs(title = "Top 3 ETFs With Best 1 Year Momentum",
       subtitle = "last 2 years of data",
       caption = "datasource: www.yahoo.com",
       x = NULL,
       y = "Momentum %",
       colour = "ETF") + 
  theme_minimal() +
  theme(plot.title = element_text(colour = "#3498DB", face = "bold"), 
        plot.subtitle = element_text(colour = "#555555", face = "bold"), 
        plot.caption = element_text(face = "italic"), 
        aspect.ratio = 9/16)

best_3_plot
```
```{r, 'CSV-Export', echo = TRUE, eval = FALSE}
# CSV Export -------------------------------------------------------------------
write.csv(leaderboard_data, "dm_etf_leaderboard.csv")
write.csv(temp_df, "dm_etf_all_2_years_data.csv")
```
```{r, 'Google-Export', echo = TRUE, eval = FALSE}
# Google Sheet Export ----------------------------------------------------------
gs4_auth()
my_gsheets <- gs4_get(Sys.getenv("file_id"))

df_to_write <- leaderboard_data
range_write(
  my_gsheets,
  df_to_write,
  sheet = 1,
  range = "one_year_momentum!A2:Z11",
  col_names = FALSE,
  reformat = TRUE
)

df_to_write <- wc_for_sql
range_write(
  my_gsheets,
  df_to_write,
  sheet = "chart_data",
  range = NULL,
  col_names = TRUE,
  reformat = TRUE
)

################################################################################
############################### END OF  MODULE I ###############################
################################################################################
```
# Additional Info

## Issue Register

Usually I don't show this part to stakeholders as it is not a part of any report but my experience as project manager taught me how important it is to have such a register.

If you are familiar with issue registers this one is quite different because it is only for me so there is no need for the owner, no dates (as I put things in it as they happen) etc. I can even say it is something between the issue register and project log.

---

1. I need to hide my credentials somehow to not commit them to the public by accident.
2. There are big differences in how Google Colab and RStudio execute code. I need to build two versions of the code.

**Solution:** I am going to use version control only in RStudio (with GitHub) and I will share my Google Colab only with people I trust.

---

3. There are going to be too many source files to keep them in Colab.

**Solution:** After consideration, I think that there is no real need to keep them in Colab. I am going to put a link to my GitHub repository for whose who are interested.

---

4. Is it really necessary to let readers interact with my data host?

**Solution:** No. But without that the code will lose a lot of its demonstrative power (to show my skills) and I don't think that the customers will abuse the database. Anyway I can always disable the connection. Risks to benefits in costs of keeping the feature.

---

5. After finishing the code I can easily gather data for all 16 cases of Double-Momentum use. Now I can focus on the next step but which one?

**Solution:** I think that creating a code for portfolio optimization will be easier from the all remaining tasks and it allows me to get the last (17th) set of data for comparison. So the next step will be module III.

---

6. New Year's Day.

**Solution:** Issue solved but I need a day of rest. (Current timestamp 2023-01-01 00:47:22) ðŸ˜ƒðŸ¥‚

---

7. There is a problem connecting to the DB. It works in Google Colab but fails in RStudio. It demands the use of SSL but when I put â€œsslmode = â€˜requireâ€™â€ it crashes the environment in RStudio.

**Solution:** Switched from RPostgreSQL to RPostgres library. It allows SSL without any problem.

---

8. I transformed the momentum calculation from scientific format too early. I need them only for data presentation (the last step) but in the DB it should be stored with big precision.

**Solved 2023-01-10:** Code in all files corrected.

---

9. I should have put the dates in this issue register. Without them I have lost a lot of explanatory power in this notebook. I missed the point that I am not the only stakeholder of this project anymore (readers are too).

**Solution:** From now on I am going to put opening and closing dates in here.

---

**2023-01-03**
10. I forgot to save the calculation results for absolute momentum.I am going to need them in Module II for backtesting.

**!! ONGOING !!**

---

**2023-01-03**
11. I need to include CSV files with my database dump for those who don't want to use databases or maybe I should only just describe a table structure for the code to work?

**Solved 2023-01-11:** Files already on GitHub. File list updated.

---

**2023-01-05**

12. There are many too many code files. I need to rewrite them in the way that one file will generate data using monthly and weekly intervals in one go.

**Solved 2023-01-11** I have rewritten the code and reduced the files by the half. File list in this document has been updated too.

---


## Project files on GitHub

*.gitignore* â€“ internal GitHub configuration file

*Analytics_Notebook.Rmd* â€“ The Projectâ€™s notebook, explanation and comments.

*project.Rproj* â€“ Rstudio Project file

*README.md* - Short description for the first contact

*tests.R* â€“ for testing new ideas and experimenting

*dm_classic_db.csv* - what is in the database for classic dual momentum

*dm_10_etfs_db.csv* - database for the 10 ETFs case

*dm_crypto_db.csv* - database content for crypto case


**Files beneath are used for data mining (1 year momentum calculation)**

*dm_classic_with_ema.R* â€“ classic ETF set, absolute momt. using EMA(30), weekly and  monthly interval

*dm_classic_with_zero.R* â€“ classic ETF set, abs. momt. using 0, weekly and  monthly interval

*dm_etfs_with_ema.R* â€“ 10 ETFs, abs. momt. using EMA(30), weekly and  monthly interval

*dm_etfs_with_zero.R* â€“ 10 ETFs, abs. momt. using 0, weekly and  monthly interval

*dm_crypto_with_ema.R* â€“ basket of crypto, abs. momt. using EMA(30), weekly and  monthly interval

*dm_crypto_with_zero.R* â€“ basket of crypto, abs. momt. using 0, weekly and monthly interval