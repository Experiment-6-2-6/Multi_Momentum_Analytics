---
title: "Dual Momentum Analysis"
author: "Kristoff Dudek"
date: "2022-11-26"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: TRUE
---

*This document serves as a record of my analytical methods and workflow, as well as a showcase of my data analysis abilities for potential clients and employers.*

*It must be noted that the information provided is for informational purposes only and does not constitute investment advice.*

---

This document is available in English, Spanish and Polish.

---

# Introduction

In this research, I propose an adaptation of the Dual-Momentum investment strategy as outlined by Gary Antonacci in his book "Dual Momentum Investing: An Innovative Strategy for Higher Returns with Lower Risk" (16 Dec. 2014 McGraw Hill). The strategy is also explained in a short YouTube video by the author, which can be found [here](https://www.youtube.com/watch?v=lf1u4dsHavs).

My objective is to investigate the potential of modifying the Dual-Momentum strategy to include crypto assets or single stocks pre-filtered by various financial ratios such as ROIC, EPS growth, BV growth, REV growth, and Debt. In order to achieve this, the analysis will be conducted in several independent yet comprehensive modules.

In addition to the primary objective, this research also serves as a demonstration of my analytical skills to potential clients and employers. Thus, the document has been designed to be comprehensive and tutorial-like in nature.

In order to thoroughly evaluate the proposed modifications to the Dual-Momentum strategy, an external database is utilized. While this may not be strictly necessary for the analysis, the use of a database allows for the demonstration of my proficiency in SQL and provides a comprehensive data source for the subsequent creation of Tableau and MS Power BI presentations.

For those interested in the practical application of this research, a version-controlled GitHub repository has been created to house the project. The repository, which can be accessed [here](https://github.com/Experiment-6-2-6/Multi_Momentum_Analytics), contains the necessary code and resources to replicate the analysis.

With the introduction now complete, let us delve into the project at hand.

# Research Questions

The goal of this research is to investigate the potential for improving upon the Dual-Momentum strategy as proposed by Gary Antonacci and to test its applicability to different assets. The current strategy has certain weaknesses, such as its tendency to miss price movements in highly volatile environments due to the monthly check-in. The following questions will guide the analysis:

1. How does the performance of the Dual-Momentum strategy change when checking momentum on a weekly basis rather than monthly?
1. What impact does this change have on turnover and trading costs?
1. Is the strategy more effective when applied to a larger number of ETFs?
1. Can the drawdown be minimized by using a 30-week EMA as an absolute momentum measure?
1. How does the strategy compare to a basket of ETFs that is optimized for performance?
1. How does the strategy perform when applied to a basket of single stocks pre-filtered by financial ratios?
1. How does the strategy perform when applied to crypto assets?
1. What is the effect of using monthly momentum rather than yearly momentum?

# Assumptions and goals

* The research aims to test the potential improvements of Gary Antonacci's Dual-Momentum investment strategy, as well as its applicability to different asset classes such as crypto and single stocks filtered by specific financial metrics.
* Data for the analysis will be sourced from publicly available, free sources and the analysis will be conducted on weekends only. The data used in the research will not be older than April 18, 2013 as this is when commission-free trading became available.
* To eliminate additional variables, all prices for ETFs, shares, and crypto will be in USD.

The research will involve data mining to obtain a table of weekly performances including the standard Dual Momentum strategy, as well as its application to baskets of 10 ETFs, 5 shares, and 10 crypto assets. Each of these options will be evaluated with both monthly and weekly rebalancing, and variants with absolute momentum calculated using both positive or negative sign and Exponential Moving Average of 30 weeks. This results in a total of 16 combinations, plus one additional for the performance of an optimized basket of ETFs.

To achieve these goals, R scripts will be prepared for: Basic Dual Momentum Assessment and use of EMA(30) (module I), Portfolio Performance Valuation (module II), Portfolio Optimization (module III), and Share Screen (module IV). The summary for module I will be presented in the form of a leaderboard table with ETFs sorted by strongest momentum. Additionally, weekly ETF momentum calculations will be added to a Google Sheet (for my personal use). The research will focus on weekly data, but daily data and calculations will also be kept in a database for use in Tableau and Power BI.

## Assets by Grups Used in the research

### Classic Dual-Momentum Case:

1. **VOO** Vanguard S&P 500 ETF
1. **VEO** Vanguard All-World ex-US ETF
1. **BND** Vanguard Total Bond Market Index ETF

### 10 ETFs from London Stock Exchange

1. **VUAA**	Vanguard S&P 500 UCITS ETF USD (acc)
1. **CNDX**	iShares NASDAQ 100 UCITS ETF USD (acc)
1. **VWRA**	Vanguard FTSE All-World UCITS ETF USD (acc)
1. **IWMO**	iShares Edge MSCI World Momentum Factor UCITS ETF USD (acc)
1. **IWQU**	iShares Edge MSCI World Quality Factor UCITS ETF USD (acc)
1. **IWVL**	IShares Edge MSCI World Value Factor UCITS ETF USD (acc)
1. **WSML**	iShares MSCI World Small Cap UCITS ETF USD (acc)
1. **VFEA**	Vanguard FTSE Emerging Mkts UCITS ETF USD (acc)
1. **IGIL**	iShares Global Inflation Linked Govt Bonds UCITS ETF USD (acc)
1. **IGLN**	iShares Physical Gold ETC USD (no income)

### Crypto Set

1. **AAVE** - AAVE Liquidity Protocol
1. **ADA**  - Cardano Token
1. **AGIX** -  SingularityNET Token
1. **ATOM** - Cosmos Network Token
1. **AVAX** - Avalanche
1. **BNB** - Binance Coin
1. **BTC** - Bitcoin
1. **CAKE** - PancakeSwap Pancake Token
1. ~~**CATGIRL** - CatGirl~~ Removed 2023-01-15 - too small quotes
1. **DOGE** - DogeCoin
1. **DOT** - PolkaDot Token
1. **ETH** - Ethereum
1. **FIL** - FileCoin
1. **LINK** - ChainLink Token
1. **LTC** - LiteCoin
1. **MANA** - Decentraland
1. **MATIC** - Poligon Token
1. **NEAR** - NEAR Protocol
1. **PAXG** - PAX Gold
1. **RNDR** - Render Token
1. ~~**SHIB** - Shiba Inu~~ Removed 2023-01-15 - too small quotes
1. **SUSHI** - SushiSwap Token
1. **TON** - TON Coin (Telegram)
1. **TRX** - TRON (BitTorrent)
1. ~~**UNI** - UniSwap Unicorn Token~~ Removed 2023-01-15 - unrelaible data

# Methodology and Workflow

The final version of the code and workflow for each module will be explained in the following paragraphs. In order to fully understand the problems and ideas addressed in the research, it is recommended to first review the "Issue Register" which serves as a chronological index of all problems, issues, ideas, and changes in the project.

## Module One: Data Mining

In Module One, data mining was conducted using a set of 10 ETFs with a 30 week exponential moving average.

### Environment Requirements

The following packages were utilized in this module: 
* Quantmod for quantitative analysis
* Tidyverse for general data manipulation
* Dplyr which is a part of Tidyverse but needs to be activated separately
* RPostgres for working with databases (do not use RPostgreSQL as it causes problems with a SSL connections)
* GT to create a Great Table view in final report
* Reshape2 to transform data into long format to simplify plot drawing
* Ggplot2 for plotting how to take over the world in 3 easy steps ðŸ¤£
* Googlesheets4 to write to my Google sheet

```{r, 'libraries', echo = TRUE, message = FALSE, warning = FALSE}
library(quantmod)
library(tidyverse)
library(dplyr)
library(RPostgres)
library(gt)
library(reshape2)
library(ggplot2)
library(googlesheets4)
```

### Getting and cleaning data

* Establishing the connection with my database
* Getting the list of the ETFs names and symbols from the db
* Limiting data pull to the last 3 years
* Populating the daily and weekly data frames:
  + getting data from Yahoo Finance
  + extracting only the adjusted daily closes
  + naming the columns with ETF names
  + dropping any rows with NA
  + transforming row names into date column (for use in SQL)
* Rewriting the quotes_etfs_x database as I want to have 3 years of daily closing prices on my server for the later use.

```{r, 'Connection-On', echo = TRUE, message = FALSE, warning = FALSE}
my_postgr <- dbConnect(
  RPostgres::Postgres(),
  host = Sys.getenv("sql_host"),
  dbname = Sys.getenv("sql_db_name"),
  user = Sys.getenv("sql_user"),
  password = Sys.getenv("sql_password"),
  port = 5432,
  sslmode = "require"
)
```
```{r, 'Getting-Data', echo = TRUE, message = FALSE, warning = FALSE}
start_date <- Sys.Date() - (3 * 365)
end_date <- Sys.Date()
etf_qts_env <- new.env()
etf_query <- dbGetQuery(my_postgr, "SELECT * FROM etf_info;")
yahoo_symbols <- as.vector(etf_query[["ticker"]])

# daily quotes
getSymbols(yahoo_symbols, 
           from = start_date, 
           to = end_date, 
           periodicity = "daily",
           env = etf_qts_env)
daily_closes <- do.call(merge, eapply(etf_qts_env, Ad))
daily_closes <- na.omit(daily_closes)
names(daily_closes) <- gsub(".L.Adjusted", "", names(daily_closes))

# weekly quotes
getSymbols(yahoo_symbols, 
           from = start_date, 
           to = end_date, 
           periodicity = "weekly",
           env = etf_qts_env)
weekly_closes <- do.call(merge, eapply(etf_qts_env, Ad))
weekly_closes <- na.omit(weekly_closes)
names(weekly_closes) <- gsub(".L.Adjusted", "", names(weekly_closes))

# monthly quotes
getSymbols(yahoo_symbols, 
           from = start_date, 
           to = end_date, 
           periodicity = "monthly",
           env = etf_qts_env)
monthly_closes <- do.call(merge, eapply(etf_qts_env, Ad))
monthly_closes <- na.omit(monthly_closes)
names(monthly_closes) <- gsub(".L.Adjusted", "", names(monthly_closes))
```
```{r, 'Quotes-to-DB', echo = TRUE, message = FALSE, warning = FALSE}
# Have to transform into PostgreSQL format before sending to DB
dc_for_sql <- as.data.frame(daily_closes)
dc_for_sql <- cbind(date = as.Date(rownames(dc_for_sql)), dc_for_sql)
if (dbExistsTable(my_postgr, "quotes_etfs_d")) {
  dbRemoveTable(my_postgr, "quotes_etfs_d")
}
dbWriteTable(my_postgr, "quotes_etfs_d", dc_for_sql, row.names = FALSE)

wc_for_sql <- as.data.frame(weekly_closes)
wc_for_sql <- cbind(date = as.Date(rownames(wc_for_sql)), wc_for_sql)
if (dbExistsTable(my_postgr, "quotes_etfs_w")){
  dbRemoveTable(my_postgr, "quotes_etfs_w")
}
dbWriteTable(my_postgr, "quotes_etfs_w", wc_for_sql, row.names = FALSE)

mc_for_sql <- as.data.frame(monthly_closes)
mc_for_sql <- cbind(date = as.Date(rownames(mc_for_sql)), mc_for_sql)
if (dbExistsTable(my_postgr, "quotes_etfs_m")){
  dbRemoveTable(my_postgr, "quotes_etfs_m")
}
dbWriteTable(my_postgr, "quotes_etfs_m", mc_for_sql, row.names = FALSE)
```

### Getting and cleaning data
* Establishing the connection with my database
* Getting the list of the ETFs names and symbols from the db
* Limiting data pull to the last 3 years
* Populating the daily and weekly data frames:
  + getting data from Yahoo Finance
  + extracting only the adjusted daily closes
  + naming the columns with ETF names
  + dropping any rows with NA
  + transforming row names into date column (for use in SQL)
* Rewriting the quotes_etfs_x database as I want to have 3 years of daily closing prices on my server for the later use.

### Calculations

* checking if the asset's price is above the EMA(30)
* calculating 1 year ROC
* Sending the results to the database.

```{r, 'Absolute-Mmt', echo = TRUE, message = FALSE, warning = FALSE}
# weekly check
abs_mmt_state_w <- rep(TRUE, ncol(weekly_closes))
names(abs_mmt_state_w) <- names(weekly_closes)
for (ticker in names(weekly_closes)){
  ema30w <- EMA(weekly_closes[,ticker], n = 30)
  if (ema30w[nrow(ema30w)] < weekly_closes[nrow(weekly_closes),ticker]) {
    abs_mmt_state_w[[ticker]] <- TRUE
  } else {
    abs_mmt_state_w[[ticker]] <- FALSE
  }
}
# monthly check
abs_mmt_state_m <- rep(TRUE, ncol(monthly_closes))
names(abs_mmt_state_m) <- names(monthly_closes)
for (ticker in names(monthly_closes)){
  ema7m <- EMA(monthly_closes[,ticker], n = 7)
  if (ema7m[nrow(ema7m)] < monthly_closes[nrow(monthly_closes),ticker]) {
    abs_mmt_state_m[[ticker]] <- TRUE
  } else {
      abs_mmt_state_m[[ticker]] <- FALSE
    }
}
```
```{r, 'Relative-Mmt', echo = TRUE, message = FALSE, warning = FALSE}
daily_1ym <- ROC(daily_closes, n = 252, type = "discrete") %>%  na.omit()
weekly_1ym <- ROC(weekly_closes, n = 50, type = "discrete") %>% na.omit()
monthly_1ym <- ROC(monthly_closes, n = 12, type = "discrete") %>% na.omit()
```
```{r, 'Momentum-to-DB', echo = TRUE, message = FALSE, warning = FALSE}
dm_for_sql <- as.data.frame(daily_1ym)
dm_for_sql <- cbind(date = as.Date(rownames(dm_for_sql)), dm_for_sql)
if (dbExistsTable(my_postgr, "momentum_etfs_d")) {
  dbRemoveTable(my_postgr, "momentum_etfs_d")
}
dbWriteTable(my_postgr, "momentum_etfs_d", dm_for_sql, row.names = FALSE)

wm_for_sql <- as.data.frame(weekly_1ym)
wm_for_sql <- cbind(date = as.Date(rownames(wm_for_sql)), wm_for_sql)
if (dbExistsTable(my_postgr, "momentum_etfs_w")) {
  dbRemoveTable(my_postgr, "momentum_etfs_w")
}
dbWriteTable(my_postgr, "momentum_etfs_w", wm_for_sql, row.names = FALSE)

mm_for_sql <- as.data.frame(monthly_1ym)
mm_for_sql <- cbind(date = as.Date(rownames(mm_for_sql)), mm_for_sql)
if (dbExistsTable(my_postgr, "momentum_etfs_m")) {
  dbRemoveTable(my_postgr, "momentum_etfs_m")
}
dbWriteTable(my_postgr, "momentum_etfs_m", mm_for_sql, row.names = FALSE)
```
```{r, 'Connection-Off', echo = TRUE, message = FALSE, warning = FALSE}
dbDisconnect(my_postgr)
```

### Table and Plot (Report)

Now it is time to build the report.
As I already have sent all necessary data to the DB I can close the connection and focus on the local data frames.

I want the report to be in the form of a table and a graph. To get that I am:

* Sorting columns by the momentum of the last week in descending order.
* Preparing the leaderboard:
  * Building the table from the components
  * Transposing the momentum table to vertical view
  * Formatting and adding columns names
  * Printing the table
* Preparing the last year momentum chart
  * Transforming selected data into long format
  * Building the plot
  * Drawing it
  
```{r, 'Table', echo = TRUE, message = FALSE, warning = FALSE}
# preparing the weekly table
temp_df_w <- as_tibble(weekly_1ym)
temp_df_w <- temp_df_w[,order(temp_df_w[nrow(temp_df_w),],decreasing = TRUE)]
temp_df_w <- tail(temp_df_w, 1) %>% "*"(100) %>% round(2)

descriptions <- NULL
for (ticker in names(temp_df_w)) {
  description <- etf_query$name[etf_query$ticker_usd == ticker]
  descriptions <- append(descriptions, description)
}

Above_ema_w <- rep(TRUE, ncol(temp_df_w))
names(Above_ema_w) <- names((temp_df_w))
for (ticker in names(temp_df_w)){
  Above_ema_w[ticker] <- abs_mmt_state_w[ticker]
}

leaderboard_data_w <- tibble(descriptions,
                             names(temp_df_w),
                             Above_ema_w,
                             t(temp_df_w))

names(leaderboard_data_w) <- c("etf_name",
                               "USD", "Above_EMA", "Y_Mm")

leaderboard_table_w <-  
  gt(leaderboard_data_w) %>% 
  tab_header(title = md("**ETFs Sorted By 1 Year Momentum**"), 
             subtitle = "last 2 years of data (weekly)") %>% 
  tab_source_note(md("_datasource: www.yahoo.com_")) %>% 
  tab_style(style = list(cell_text(color = "#196F3D")),
            locations = cells_body(columns = Above_EMA, 
                                   rows = Above_EMA == TRUE)) %>% 
  tab_style(style = list(cell_text(color = "#7B241C")),
            locations = cells_body(columns = Above_EMA, 
                                   rows = Above_EMA == FALSE)) %>% 
  tab_style(style = list(cell_fill(color = "#E8F8F5"),
                         cell_text(weight =  "bold")),
            locations = cells_body(rows = 1)) %>% 
  tab_style(style = cell_text(align = "right"),
            locations = cells_source_notes()) %>%
  cols_label(etf_name = "ETF Full Name / Description", 
             USD = "Ticker",
             Above_EMA = "Above EMA", 
             Y_Mm = "1Y Mm %")

# preparing the monthly table
temp_df_m <- as_tibble(monthly_1ym)
temp_df_m <- temp_df_m[,order(temp_df_m[nrow(temp_df_m),],decreasing = TRUE)]
temp_df_m <- tail(temp_df_m, 1) %>% "*"(100) %>% round(2)

descriptions <- NULL
for (ticker in names(temp_df_m)) {
  description <- etf_query$name[etf_query$ticker_usd == ticker]
  descriptions <- append(descriptions, description)
}

Above_ema_m <- rep(TRUE, ncol(temp_df_m))
names(Above_ema_m) <- names((temp_df_m))
for (ticker in names(temp_df_m)){
  Above_ema_m[ticker] <- abs_mmt_state_m[ticker]
}

leaderboard_data_m <- tibble(descriptions,
                           names(temp_df_m),
                           Above_ema_m,
                           t(temp_df_m))

names(leaderboard_data_m) <- c("etf_name",
                             "USD", "Above_EMA", "Y_Mm")

leaderboard_table_m <-  
  gt(leaderboard_data_m) %>% 
  tab_header(title = md("**ETFs Sorted By 1 Year Momentum**"), 
             subtitle = "last 2 years of data (monthly)") %>% 
  tab_source_note(md("_datasource: www.yahoo.com_")) %>% 
  tab_style(style = list(cell_text(color = "#196F3D")),
    locations = cells_body(columns = Above_EMA, rows = Above_EMA == TRUE)) %>% 
  tab_style(style = list(cell_text(color = "#7B241C")),
    locations = cells_body(columns = Above_EMA, rows = Above_EMA == FALSE)) %>% 
  tab_style(style = list(cell_fill(color = "#E8F8F5"),
                         cell_text(weight =  "bold")),
    locations = cells_body(rows = 1)) %>% 
  tab_style(style = cell_text(align = "right"),
    locations = cells_source_notes()) %>%
  cols_label(etf_name = "ETF Full Name / Description", 
             USD = "Ticker",
             Above_EMA = "Above EMA", 
             Y_Mm = "1Y Mm %")

leaderboard_table_w
leaderboard_table_m
```
```{r, 'Plot', echo = TRUE, warning = FALSE, out.width= '100%', fig.asp = '1.618', dpi = 96}
# momentum plot weekly
temp_df_w <- select(wm_for_sql, -date)
temp_df_w <- temp_df_w[,order(temp_df_w[nrow(temp_df_w),],decreasing = TRUE)]
temp_df_w <- select(temp_df_w, 1:3) %>% "*"(100) %>% round(2)
temp_df_w <- tibble(date = wm_for_sql$date, temp_df_w)
temp_df_w <- tail(temp_df_w, 100)
plot_data_w <- melt(temp_df_w, id = "date")

best_3_plot_w <- ggplot(plot_data_w, 
                        aes(x = date, y = value, colour = variable)) +
  geom_line() +
  geom_hline(yintercept = 0) + 
  labs(title = "ETFs by the best 1 Year Momentum",
       subtitle = "last 2 years of data (weekly)",
       caption = "datasource: www.yahoo.com",
       x = NULL,
       y = "Momentum %",
       colour = "ETF") + 
  theme_minimal() +
  theme(plot.title = element_text(colour = "#3498DB", face = "bold"), 
        plot.subtitle = element_text(colour = "#555555", face = "bold"), 
        plot.caption = element_text(face = "italic"), 
        aspect.ratio = 9/16)

# momentum plot monthly
temp_df_m <- select(mm_for_sql, -date)
temp_df_m <- temp_df_m[,order(temp_df_m[nrow(temp_df_m),],decreasing = TRUE)]
temp_df_m <- select(temp_df_m, 1:3) %>% "*"(100) %>% round(2)
temp_df_m <- tibble(date = mm_for_sql$date, temp_df_m)
temp_df_m <- tail(temp_df_m, 24)
plot_data_m <- melt(temp_df_m, id = "date")

best_3_plot_m <- ggplot(plot_data_m, 
                        aes(x = date, y = value, colour = variable)) +
  geom_line() +
  geom_hline(yintercept = 0) + 
  labs(title = "ETFs by the best 1 Year Momentum",
       subtitle = "last 2 years of data (monthly)",
       caption = "datasource: www.yahoo.com",
       x = NULL,
       y = "Momentum %",
       colour = "ETF") + 
  theme_minimal() +
  theme(plot.title = element_text(colour = "#3498DB", face = "bold"), 
        plot.subtitle = element_text(colour = "#555555", face = "bold"), 
        plot.caption = element_text(face = "italic"), 
        aspect.ratio = 9/16)

best_3_plot_w
best_3_plot_m
```

### Exporting CSV file.

In order to facilitate data exploration and manipulation, I have implemented the capability to export the data in csv format. This makes it simple for stakeholders to access the data and perform their own analysis using tools like Excel. Furthermore, it allows for easy integration with other data sources for more in-depth examination.

```{r, 'CSV-Export', echo = TRUE, eval = FALSE}
write.csv(leaderboard_data_w, "dm_10_etfs_weekly.csv")
write.csv(leaderboard_data_m, "dm_10_etfs_monthly.csv")
write.csv(temp_df_w, "dm_10_etfs_2_years_of_mmt_data_weekly.csv")
write.csv(temp_df_m, "dm_10_etfs_2_years_of_mmt_data_monthly.csv")
```

### Adding output to Google Sheet.

For my convenience and accessibility, I have exported the dataframes into a Google Spreadsheet. This can be accessed by clicking [here](https://docs.google.com/spreadsheets/d/1ULTKECt44zBzhROLlXMx3XtD-Vm0M5kQ2Ve63AtzJ64/edit?usp=sharing). The spreadsheet includes a summary table and a hidden sheet, which is used to create a chart on a separate, visible sheet. It is important to note that while this feature is included for convenience, running this specific code chunk is not necessary for the analysis.

```{r, 'Google-Export', echo = TRUE, eval = FALSE}
gs4_auth()
my_gsheets <- gs4_get(Sys.getenv("file_id"))

df_to_write <- leaderboard_data_w
range_write(
  my_gsheets,
  df_to_write,
  sheet = 1,
  range = "one_year_momentum!A2:Z11",
  col_names = FALSE,
  reformat = TRUE
)

df_to_write <- wc_for_sql
range_write(
  my_gsheets,
  df_to_write,
  sheet = "chart_data",
  range = NULL,
  col_names = TRUE,
  reformat = TRUE
)
```

## Issue Register

As a data analyst with project manager experience, I use some tools specific to PM. This is why I use a form of an "Issue Register". It serves as a log of any problems, ideas, changes, and other relevant information that occurs throughout the project.

It is worth noting that this register is for my own use only and it is not my standard practice to share it with stakeholders as it is not a part of any official report. It serves as a tool for me to keep track of progress and address any issues that may arise. Additionally, the register does not include specific dates as I add entries as they happen. It can be considered a combination of an issue register and a project log.

Please note that this register is primarily intended for my own use and therefore the language used may be more direct and concise. In any doubt please ask questions by the email.

---

1. To keep my DB credentials secure, I will need to find a way to hide them and prevent committing them to the public by accident.
2. There are significant differences in how Google Colab and RStudio execute code. To account for this, I will need to create separate versions of the code for each platform.

**Solved**

To maintain version control, I will only use RStudio in conjunction with GitHub. 
To show the DB capabilities of the code I will share it with with all credentials on Google Colab but only with trusted individuals.

---

3. There are going to be too many source files to keep them in Colab.

**Solved**

After consideration, I think that there is no real need to keep them in Colab. I am going to put a link to my GitHub repository for those who are interested.

---

4. Is it really necessary to let readers interact with my data host?

**Solved**

No. But without that the code will lose a lot of its demonstrative power (to show my skills) and I don't think that the customers will abuse the database. Anyway I can always disable the connection. Risks to benefits in costs of keeping the feature.

---

5. After finishing the code I can easily gather data for all 16 cases of Double-Momentum use. Now I can focus on the next step but which one?

**Solved**

I think that creating a code for portfolio optimization will be easies from the remaining tasks and it allows me to get the last (17th) set of data for comparison. So the next step will be module III.

---

6. New Year's Day.

**Solved**

Issue solved but I need a day of rest. (Current timestamp 2023-01-01 00:47:22 UTC) ðŸ˜ƒðŸ¥‚

---

7. There is a problem connecting to the DB. It works in Google Colad but fails in RStudio. It demands the use of SSL but when I put â€œsslmode = â€˜requireâ€™â€ it crashes the environment in RStudio.

**Solved**

Switched from RPostgreSQL to RPostgres library. It allows SSL without any problem.

---

8. I transformed the momentum calculation from scientific format too early. I need them only for data presentation (the last step) but in the DB it should be stored with big precision.

**Solved [2023-01-10]**

Code in all files corrected.

---

9. I should have put the dates in this issue register. Without them I have lost a lot of explanatory power in this notebook. I missed the point that I am not the only stakeholder of this project anymore (readers are too).

**Solved**

From now on I am going to put opening and closing dates in here.

---

**2023-01-03**

10. I forgot to save the calculation results for absolute momentum. I am going to need them in Module II for backtesting.

2023-01-12
I think I need to do this in a different file. I have already put a lot into the "Module I" and I don't want to put all the work (and code) into one file. It can unnecessarily complicate future maintenance.

**Solved [2023-01-14]**

Decision. I am going to keep data mining together so I will build datatables for methods comparison. It will probably be something like: Asset name, Date, Quotation, Absolute Momentum (T/F), Momentum levels for comparison. (Long format)

---


**2023-01-03**

11. I need to include CSV files with my database dump for those who don't want to use databases... or maybe I should only just describe a table structure for the code to work?

**Solved [2023-01-11]**

Files already on GitHub. File list updated.

---

**2023-01-05**

12. There are many too many code files. I need to rewrite them in the way that one file will generate data using monthly and weekly intervals in one go.

**Solved [2023-01-11]**

I have rewritten the code and reduced the files by the half. File list in this document has been updated too.

---

**2023-01-15**

13. In the files for crypto the momentum calculations for UNI-USD are absurdly high. It looks like there is some bug in data but I need to take a closer took, just in case.

**Solved [2023-01-15]**

I have discovered that there are 3 tokens which have problems with quotation. I have already more than 20 cryptos on the list so I am going to remove the "trouble makers". Reason: I don't want to loose a tone of time in a search of reliable data source and 22 crypyos are enough for me.

---

**2023-01-15**

14. What a blunder! I have just discovered that I have one misaligned column in my leader's table. I have sorted all assets by the momentum but I had forgotten to re-order  the absolute momentum calculations to match the rest. Oof!

**Solved [2023-01-15]**

All code files corrected.

---

## Project files on GitHub

***.gitignore*** â€“ internal GitHub configuration file

***Analytics_Notebook.html*** â€“ The Projectâ€™s notebook fully generated and ready.

***Analytics_Notebook.Rmd*** â€“ The Projectâ€™s notebook R Markdown source code.

***project.Rproj*** â€“ Rstudio Project file

***README.md*** - Short description for the first contact

***tests.R*** â€“ for testing new ideas and experimenting

***dm_classic_db.csv*** - what is in the database for classic dual momentum

***dm_10_etfs_db.csv*** - database for the 10 ETFs case

***dm_crypto_db.csv*** - database content for crypto case

**Files beneath are used for data mining (1 year momentum calculation)**

***dm_classic_with_ema.R*** â€“ classic ETF set, absolute momt. using EMA(30), weekly and  monthly interval

***dm_classic_with_zero.R*** â€“ classic ETF set, abs. momt. using 0, weekly and  monthly interval

***dm_etfs_with_ema.R*** â€“ 10 ETFs, abs. momt. using EMA(30), weekly and  monthly interval

***dm_etfs_with_zero.R*** â€“ 10 ETFs, abs. momt. using 0, weekly and  monthly interval

***dm_crypto_with_ema.R*** â€“ basket of crypto, abs. momt. using EMA(30), weekly and  monthly interval

***dm_crypto_with_zero.R*** â€“ basket of crypto, abs. momt. using 0, weekly and monthly interval